{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40eb264d-1e1f-45d2-a307-e3cf629a7025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37bc35f9-9d95-46de-8709-6fb05efc52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"glaiveai/glaive-function-calling-v2\"\n",
    "bos_token, eos_token = \"<s>\",\"</s>\"\n",
    "bop_token, eop_token = \"[INST]\",\"[/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e855fd-9eb6-4192-a478-7ec874c9bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset = load_dataset(dataset_id,split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96171ba-26fa-4539-9694-b240bbfa3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(record):\n",
    "    system = record[\"system\"] \n",
    "\n",
    "    \"\"\"\n",
    "    record[\"system\"] = system.replace(\"SYSTEM:\",\"System:\")\n",
    "    \"\"\"\n",
    "    \n",
    "    chat = record[\"chat\"]\n",
    "    \n",
    "    chat = (chat\n",
    "            #.replace(\"USER:\",\"Human:\")\n",
    "            .replace(\"ASSISTANT: <functioncall>\",\"FUNCTION:\")\n",
    "            #.replace(\"ASSISTANT:\",\"AI:\")\n",
    "            #.replace(\"FUNCTION RESPONSE:\",\"Response:\")\n",
    "            .replace(\"<|endoftext|>\",\"\")\n",
    "           )\n",
    "    \n",
    "    chat = chat.split(\"\\n\\n\\n\")\n",
    "\n",
    "    record[\"prompt\"] = chat[0]\n",
    "    record[\"response\"] = \"\\n\".join(chat[1:])\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "738175ab-e80c-4231-9bf4-dd248b9edf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chat', 'system', 'prompt', 'response'],\n",
       "    num_rows: 79643\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda c: c[\"chat\"].find(\"\\n\\n\\n\") > 1)\n",
    "instruct_tune_dataset = instruct_tune_dataset.map(clean_data)\n",
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597e5129-aa37-446f-ad92-0b5f7d539464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chat', 'system', 'prompt', 'response'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 421337\n",
    "sample_size = 100\n",
    "instruct_tune_dataset = instruct_tune_dataset.shuffle(seed=seed).select(range(sample_size))\n",
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d5f419b-1a77-4b45-8374-d7d31113fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(record):\n",
    "    \n",
    "    system = record[\"system\"]\n",
    "    prompt = record[\"prompt\"]\n",
    "    response = record[\"response\"]\n",
    "\n",
    "    full_prompt = \"\"\n",
    "    full_prompt += bos_token\n",
    "    full_prompt += bop_token\n",
    "    \n",
    "    # system\n",
    "    full_prompt += system\n",
    "    \n",
    "    # prompt\n",
    "    full_prompt += \"\\n\"\n",
    "    full_prompt += prompt\n",
    "    \n",
    "    full_prompt += eop_token\n",
    "    full_prompt += \"\\n\"\n",
    "    \n",
    "    # response\n",
    "    full_prompt += response\n",
    "    full_prompt += eos_token\n",
    "    full_prompt += \"\\n\"\n",
    "    \n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83621216-db46-40eb-a022-9bc87fa412ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt(prompt):\n",
    "    idx1 = prompt.index(bop_token)\n",
    "    idx2 = prompt.index(eop_token) + len(eop_token)\n",
    "    return prompt[idx1: idx2] + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15924c72-06d6-45af-a489-1c8a4ba88b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = instruct_tune_dataset\n",
    "complete_prompts = list(map(create_prompt,sample))\n",
    "user_prompts = list(map(extract_prompt, complete_prompts))\n",
    "sample_df = pd.DataFrame({\"user_prompt\":user_prompts,\"expected\":complete_prompts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b522381-43eb-4093-b7eb-f2acd6e8049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[\"func_expected\"] = sample_df[\"expected\"].apply(lambda prompt: prompt.find(\"FUNCTION:\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8649f4c8-b61d-4dcd-b32d-5c7da9a7f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    #\"mistral:v0.2\",\n",
    "    #\"mistral-functioncalling-1-100\",\n",
    "    #\"mistral-functioncalling-1-1000\",\n",
    "    #\"mistral-functioncalling-1-5000\",\n",
    "    #\"mistral-functioncalling-1-10000\",\n",
    "    #\"mistral-functioncalling-2-5000\",\n",
    "    \"mistral-functioncalling-2-10000\",\n",
    "    #\"mistral-functioncalling-5-1000\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1e1d76-b95b-4876-88c2-a5b7332a7d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mistral-functioncalling-2-10000']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dccf551c-60bc-4782-8e5f-abe044b1d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(\"ollama_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d40cd8e1-eebd-418d-bf58-a3c2b3f3cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    # print(\"Start\",\"#\"*10)\n",
    "    # print(prompt)\n",
    "    print(\"Generating...\",\"#\"*10)\n",
    "    response = llm.invoke(prompt,num_predict=256)\n",
    "    # print(response)\n",
    "    # print(\"Done generating\",\"#\"*10)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23f4db3b-ad1e-486a-a1c2-3039253d6007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching to mistral-functioncalling-2-10000\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Generating... ##########\n",
      "Done\n",
      "##########\n",
      "CPU times: user 964 ms, sys: 119 ms, total: 1.08 s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for idx, model in enumerate(models):\n",
    "    print(f\"Switching to {model}\")\n",
    "    llm = Ollama(model=model,temperature=0)\n",
    "    sample_df[f\"{model}\"] = sample_df[\"user_prompt\"].apply(generate)\n",
    "    sample_df[f\"found_{model}\"] = sample_df[f\"{model}\"].apply(lambda prompt: prompt.find(\"FUNCTION:\") > 0)\n",
    "    print(\"Done\")\n",
    "    sample_df.to_csv(\"ollama_test.csv\",index=False)\n",
    "    print(\"#\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8904be5-1460-4760-aa2c-ef089b95ef46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "func_expected                            81\n",
       "found_mistral:v0.2                        0\n",
       "found_mistral-functioncalling-1-100       0\n",
       "found_mistral-functioncalling-1-1000      0\n",
       "found_mistral-functioncalling-1-5000      6\n",
       "found_mistral-functioncalling-1-10000    30\n",
       "found_mistral-functioncalling-2-5000     51\n",
       "found_mistral-functioncalling-2-10000    35\n",
       "found_mistral-functioncalling-5-1000      6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.loc[:,[\n",
    "    \"func_expected\",\n",
    "    \"found_mistral:v0.2\",\n",
    "    \"found_mistral-functioncalling-1-100\",\n",
    "    \"found_mistral-functioncalling-1-1000\",\n",
    "    \"found_mistral-functioncalling-1-5000\",\n",
    "    \"found_mistral-functioncalling-1-10000\",\n",
    "    \"found_mistral-functioncalling-2-5000\",\n",
    "    \"found_mistral-functioncalling-2-10000\",\n",
    "    \"found_mistral-functioncalling-5-1000\"\n",
    "]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73e5ab65-6152-4bff-96df-f7d40f4de302",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(\"ollama_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916560c-923f-4fda-88cc-1670d33af452",
   "metadata": {},
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc16772-48dc-419d-ba2c-2af2b484612e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
