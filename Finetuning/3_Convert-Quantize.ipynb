{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e71606-dfe8-43ab-aae2-cee0c6ea0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "finetunes = [\n",
    "    #\"lora-2024-02-11-17-39-18-samples-100\",\n",
    "    #\"lora-2024-02-11-17-48-53-samples-1000\",\n",
    "    #\"lora-2024-02-11-17-59-25-samples-5000\",\n",
    "    #\"lora-2024-02-11-18-28-44-samples-10000\",\n",
    "    #\"lora-2024-02-11-19-41-50-samples-5000-epochs-2\",\n",
    "    #\"lora-2024-02-11-20-39-28-samples-1000-epochs-5\",\n",
    "    #\"lora-2024-02-11-22-04-12-samples-1000-epochs-10\",\n",
    "    \"lora-2024-02-12-08-53-55-samples-10000-epochs-2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20700a80-1c7f-4f89-ad55-c49c4b02ff9a",
   "metadata": {},
   "source": [
    "## Convert Model to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d6eb06-9d08-4b2f-84b2-67fbd44bd3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: convert.py [-h] [--awq-path AWQ_PATH] [--dump] [--dump-single]\n",
      "                  [--vocab-only] [--outtype {f32,f16,q8_0}]\n",
      "                  [--vocab-dir VOCAB_DIR] [--vocab-type {spm,bpe,hfft}]\n",
      "                  [--outfile OUTFILE] [--ctx CTX] [--concurrency CONCURRENCY]\n",
      "                  [--big-endian] [--pad-vocab]\n",
      "                  model\n",
      "convert.py: error: the following arguments are required: model\n"
     ]
    }
   ],
   "source": [
    "!python ~/Projects/llama.cpp/convert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b122fe2-b5e1-47c7-8ee7-7ca79605891f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file /home/karim/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/model-00001-of-00003.safetensors\n",
      "Loading model file /home/karim/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/model-00001-of-00003.safetensors\n",
      "Loading model file /home/karim/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/model-00002-of-00003.safetensors\n",
      "Loading model file /home/karim/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/model-00003-of-00003.safetensors\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=1000000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=PosixPath('/home/karim/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61'))\n",
      "Found vocab files: {'tokenizer.model': PosixPath('/home/karim/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': PosixPath('/home/karim/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/tokenizer.json')}\n",
      "Loading vocab file '/home/karim/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/tokenizer.model', type 'spm'\n",
      "Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32000, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | BF16   | [32000, 4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | BF16   | [4096]\n",
      "Writing mistral-7b-q8_0.gguf, format 7\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "gguf: Setting chat_template to {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type Q8_0 | T+   4\n",
      "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+   4\n",
      "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+   4\n",
      "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+   4\n",
      "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   4\n",
      "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+   4\n",
      "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+   4\n",
      "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+   4\n",
      "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+   4\n",
      "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+   5\n",
      "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+   6\n",
      "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+   6\n",
      "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   6\n",
      "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+   6\n",
      "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+   6\n",
      "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+   6\n",
      "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+   6\n",
      "[ 20/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+   7\n",
      "[ 21/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+   7\n",
      "[ 22/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+   7\n",
      "[ 23/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+   7\n",
      "[ 24/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+   7\n",
      "[ 25/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+   7\n",
      "[ 26/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+   8\n",
      "[ 28/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+   8\n",
      "[ 29/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+   8\n",
      "[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   9\n",
      "[ 31/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+   9\n",
      "[ 32/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+   9\n",
      "[ 33/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+   9\n",
      "[ 34/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+   9\n",
      "[ 35/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+   9\n",
      "[ 37/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  10\n",
      "[ 38/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  10\n",
      "[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  10\n",
      "[ 40/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  10\n",
      "[ 41/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  10\n",
      "[ 42/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  10\n",
      "[ 43/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  10\n",
      "[ 44/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  10\n",
      "[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  11\n",
      "[ 46/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  12\n",
      "[ 47/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  12\n",
      "[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  12\n",
      "[ 49/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  12\n",
      "[ 50/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 51/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 52/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  12\n",
      "[ 53/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  12\n",
      "[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  13\n",
      "[ 55/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  13\n",
      "[ 56/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  14\n",
      "[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  14\n",
      "[ 58/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  14\n",
      "[ 59/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  14\n",
      "[ 60/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  14\n",
      "[ 61/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  14\n",
      "[ 62/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  14\n",
      "[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  15\n",
      "[ 64/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  15\n",
      "[ 65/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  16\n",
      "[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  16\n",
      "[ 67/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  16\n",
      "[ 68/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  16\n",
      "[ 69/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  16\n",
      "[ 70/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  16\n",
      "[ 71/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  16\n",
      "[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  17\n",
      "[ 73/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  17\n",
      "[ 74/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  17\n",
      "[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  18\n",
      "[ 76/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  18\n",
      "[ 77/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  18\n",
      "[ 78/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  18\n",
      "[ 79/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  18\n",
      "[ 80/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  18\n",
      "[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  19\n",
      "[ 82/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  19\n",
      "[ 83/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  19\n",
      "[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  19\n",
      "[ 85/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  19\n",
      "[ 86/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  19\n",
      "[ 87/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  19\n",
      "[ 88/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  19\n",
      "[ 89/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  19\n",
      "[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  20\n",
      "[ 91/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  21\n",
      "[ 92/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  21\n",
      "[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  21\n",
      "[ 94/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  21\n",
      "[ 95/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  21\n",
      "[ 96/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  21\n",
      "[ 97/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  21\n",
      "[ 98/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  21\n",
      "[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  22\n",
      "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  22\n",
      "[101/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  22\n",
      "[102/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  23\n",
      "[103/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  23\n",
      "[104/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  23\n",
      "[105/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  23\n",
      "[106/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  23\n",
      "[107/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  23\n",
      "[108/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  23\n",
      "[109/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  23\n",
      "[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  23\n",
      "[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  25\n",
      "[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  25\n",
      "[113/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  25\n",
      "[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  25\n",
      "[115/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  25\n",
      "[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[118/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  25\n",
      "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  25\n",
      "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  26\n",
      "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  27\n",
      "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  27\n",
      "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  27\n",
      "[124/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  27\n",
      "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  27\n",
      "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  27\n",
      "[127/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  27\n",
      "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  27\n",
      "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  28\n",
      "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  29\n",
      "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  29\n",
      "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  29\n",
      "[133/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  29\n",
      "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[136/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  29\n",
      "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  29\n",
      "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  30\n",
      "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  30\n",
      "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  31\n",
      "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  31\n",
      "[142/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  31\n",
      "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  31\n",
      "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  31\n",
      "[145/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  31\n",
      "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  31\n",
      "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  32\n",
      "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  32\n",
      "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  33\n",
      "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  33\n",
      "[151/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  33\n",
      "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  33\n",
      "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  33\n",
      "[154/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  33\n",
      "[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  33\n",
      "[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  34\n",
      "[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  34\n",
      "[158/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  34\n",
      "[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  35\n",
      "[160/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  35\n",
      "[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  35\n",
      "[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  35\n",
      "[163/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  35\n",
      "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  35\n",
      "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  36\n",
      "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  36\n",
      "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  36\n",
      "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  36\n",
      "[169/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  36\n",
      "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  36\n",
      "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  36\n",
      "[172/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  36\n",
      "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  36\n",
      "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  37\n",
      "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  38\n",
      "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  38\n",
      "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  38\n",
      "[178/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  38\n",
      "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  38\n",
      "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  38\n",
      "[181/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  38\n",
      "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  38\n",
      "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  39\n",
      "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  40\n",
      "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  40\n",
      "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  40\n",
      "[187/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  40\n",
      "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  40\n",
      "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  40\n",
      "[190/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  40\n",
      "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  40\n",
      "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  41\n",
      "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  42\n",
      "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  42\n",
      "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  42\n",
      "[196/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  42\n",
      "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  42\n",
      "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  42\n",
      "[199/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  42\n",
      "[200/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  42\n",
      "[201/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  42\n",
      "[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  42\n",
      "[203/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  42\n",
      "[204/291] Writing tensor output.weight                          | size  32000 x   4096  | type Q8_0 | T+  45\n",
      "[205/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  45\n",
      "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  45\n",
      "[207/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  46\n",
      "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  46\n",
      "[209/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  46\n",
      "[210/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  46\n",
      "[211/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  46\n",
      "[212/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  47\n",
      "[213/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  47\n",
      "[214/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  47\n",
      "[215/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  47\n",
      "[216/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[217/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[218/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  47\n",
      "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  47\n",
      "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  48\n",
      "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  48\n",
      "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  49\n",
      "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  49\n",
      "[224/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  49\n",
      "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  49\n",
      "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  49\n",
      "[227/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  49\n",
      "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  49\n",
      "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  50\n",
      "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  50\n",
      "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  50\n",
      "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  51\n",
      "[233/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  51\n",
      "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  51\n",
      "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  51\n",
      "[236/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  51\n",
      "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  51\n",
      "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  52\n",
      "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  52\n",
      "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  52\n",
      "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  52\n",
      "[242/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  52\n",
      "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  52\n",
      "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  52\n",
      "[245/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  52\n",
      "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  52\n",
      "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  53\n",
      "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  54\n",
      "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  54\n",
      "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  54\n",
      "[251/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  54\n",
      "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  54\n",
      "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  54\n",
      "[254/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  54\n",
      "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  54\n",
      "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  55\n",
      "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  56\n",
      "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  56\n",
      "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  56\n",
      "[260/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  56\n",
      "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  56\n",
      "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  56\n",
      "[263/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  56\n",
      "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  56\n",
      "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  57\n",
      "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  57\n",
      "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  58\n",
      "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  58\n",
      "[269/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  58\n",
      "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[272/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  58\n",
      "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  58\n",
      "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  59\n",
      "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  59\n",
      "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  60\n",
      "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  60\n",
      "[278/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  60\n",
      "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  60\n",
      "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  60\n",
      "[281/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  60\n",
      "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  60\n",
      "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  61\n",
      "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  61\n",
      "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  61\n",
      "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  61\n",
      "[287/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  61\n",
      "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[290/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  61\n",
      "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  61\n",
      "Wrote mistral-7b-q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "!python ~/Projects/llama.cpp/convert.py --outtype q8_0 --outfile mistral-7b-q8_0.gguf ~/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46e5837-a269-44dc-ba4f-1cb3d14fcbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: convert-hf-to-gguf.py [-h] [--vocab-only] [--awq-path AWQ_PATH]\n",
      "                             [--outfile OUTFILE] [--outtype {f32,f16}]\n",
      "                             [--bigendian]\n",
      "                             model\n",
      "convert-hf-to-gguf.py: error: the following arguments are required: model\n"
     ]
    }
   ],
   "source": [
    "!python ~/Projects/llama.cpp/convert-hf-to-gguf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "515c5b85-88a7-4320-8457-08132ec34435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: b70aa86578567ba3301b21c8a27bea4e8f6d6d61\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/karim/Projects/llama.cpp/convert-hf-to-gguf.py\", line 1720, in <module>\n",
      "    main()\n",
      "  File \"/home/karim/Projects/llama.cpp/convert-hf-to-gguf.py\", line 1701, in main\n",
      "    model_instance = model_class(dir_model, ftype_map[args.outtype], fname_out, args.bigendian)\n",
      "  File \"/home/karim/Projects/llama.cpp/convert-hf-to-gguf.py\", line 59, in __init__\n",
      "    self.model_arch = self._get_model_architecture()\n",
      "  File \"/home/karim/Projects/llama.cpp/convert-hf-to-gguf.py\", line 268, in _get_model_architecture\n",
      "    raise NotImplementedError(f'Architecture \"{arch}\" not supported!')\n",
      "NotImplementedError: Architecture \"MistralForCausalLM\" not supported!\n"
     ]
    }
   ],
   "source": [
    "!python ~/Projects/llama.cpp/convert-hf-to-gguf.py --outfile mistral-7b.gguf --outtype f16 ~/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6428d697-395a-4bf4-8fae-ac6ad97ccee6",
   "metadata": {},
   "source": [
    "## Convert Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a772552-8888-4e65-9062-60cc25c86faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: python /home/karim/Projects/llama.cpp/convert-lora-to-ggml.py <path> [arch]\n",
      "Path must contain HuggingFace PEFT LoRA files 'adapter_config.json' and 'adapter_model.bin'\n",
      "Arch must be one of ['llama', 'falcon', 'baichuan', 'gpt2', 'gptj', 'gptneox', 'mpt', 'starcoder', 'persimmon', 'refact', 'bert', 'bloom', 'stablelm', 'qwen', 'qwen2', 'phi2', 'plamo', 'codeshell', 'orion', 'internlm2', 'minicpm'] (default: llama)\n"
     ]
    }
   ],
   "source": [
    "!python ~/Projects/llama.cpp/convert-lora-to-ggml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d7b51d-95d8-4cc2-8cf7-3a908d54a29a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.26.self_attn.q_proj => blk.26.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.26.self_attn.q_proj => blk.26.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.26.self_attn.v_proj => blk.26.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.26.self_attn.v_proj => blk.26.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.27.self_attn.q_proj => blk.27.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.27.self_attn.q_proj => blk.27.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.27.self_attn.v_proj => blk.27.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.27.self_attn.v_proj => blk.27.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.28.self_attn.q_proj => blk.28.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.28.self_attn.q_proj => blk.28.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.28.self_attn.v_proj => blk.28.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.28.self_attn.v_proj => blk.28.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.29.self_attn.q_proj => blk.29.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.29.self_attn.q_proj => blk.29.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.29.self_attn.v_proj => blk.29.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.29.self_attn.v_proj => blk.29.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.30.self_attn.q_proj => blk.30.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.30.self_attn.q_proj => blk.30.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.30.self_attn.v_proj => blk.30.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.30.self_attn.v_proj => blk.30.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.31.self_attn.q_proj => blk.31.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.31.self_attn.q_proj => blk.31.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.31.self_attn.v_proj => blk.31.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.31.self_attn.v_proj => blk.31.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraB (1024, 64) float32 0.25MB\n",
      "Converted lora-2024-02-12-08-53-55-samples-10000-epochs-2/adapter_config.json and lora-2024-02-12-08-53-55-samples-10000-epochs-2/adapter_model.safetensors to lora-2024-02-12-08-53-55-samples-10000-epochs-2/ggml-adapter-model.bin\n"
     ]
    }
   ],
   "source": [
    "for finetune in finetunes:\n",
    "    !python ~/Projects/llama.cpp/convert-lora-to-ggml.py {finetune} llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfe04825-6303-4c1b-8a7c-72cb091e41bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 karim karim 7.2G Feb 11 23:35 mistral-7b-q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "!ls -lah *.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3b418-5fe1-4ba8-bc04-7ca601e27d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
